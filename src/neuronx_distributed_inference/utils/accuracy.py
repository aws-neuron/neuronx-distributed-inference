"""
This is a temporary file to get the testing running for new package.

Some of the utitlies functions need to be redo or removed.
"""
# flake8: noqa

from typing import List, Optional, Union, Type

import torch
from transformers import AutoTokenizer, PreTrainedModel
from transformers.generation import SampleDecoderOnlyOutput, SampleEncoderDecoderOutput


SampleOutput = Union[SampleEncoderDecoderOutput, SampleDecoderOnlyOutput]

from neuronx_distributed_inference.utils.constants import *


def get_generate_outputs(model,
             prompts,
             tokenizer,
             is_hf=False,
             draft_model=None,
             device="neuron",
             **generate_kwargs):

    if draft_model is not None:
        generate_kwargs.update(
            {
                "assistant_model": draft_model,
                "do_sample": False,
            }
        )

    tokenizer.pad_token = tokenizer.eos_token

    if is_hf:
        tokenizer.padding_side = "left"
    else:
        # FIXME: add cpu generation
        if device == "cpu":
            assert "get_generate_outputs from CPU yet avaialble"
        tokenizer.padding_side = "right"
    inputs = tokenizer(prompts, padding=True, return_tensors="pt")


    outputs = model.generate(
            inputs.input_ids,
            attention_mask=inputs.attention_mask,
            **generate_kwargs
    )

    if isinstance(outputs, SampleOutput.__args__):
        # Get token ids from output when return_dict_in_generate=True
        output_ids = outputs.sequences
    else:
        output_ids = outputs
    output_tokens = tokenizer.batch_decode(output_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)

    return output_ids, output_tokens

# FIXME: add on cpu check support
def check_accuracy(
    neuron_model: PreTrainedModel,
    expected_token_ids: Optional[List] = None,
    do_sample: bool = True,
    draft_model: PreTrainedModel = None,
    speculation_length: int = 0,
    prompt: Optional[str] = None,
    tokenizer: Optional[AutoTokenizer] = None,
    image = None,
):
    """
    Function to compare outputs from huggingface model and neuronx NxD model
    """
    neuron_config = neuron_model.neuron_config
    generation_kwargs = {
        "do_sample": do_sample,
        "max_length": neuron_config.max_length,
    }

    print(f"run accuracy check with generation_config as: {generation_kwargs}")
    if prompt is None:
        prompts = [TEST_PROMPT] * neuron_config.batch_size
    else:
        prompts = [prompt] * neuron_config.batch_size

    # FIXME: add image support

    if expected_token_ids is not None:
        outputs_expected = tokenizer.batch_decode(
            expected_token_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )
    else:
        # Generate goldens with HF on CPU
        hf_model = neuron_model.load_hf_model(neuron_model.model_path)
        output_token_ids, outputs_expected = get_generate_outputs(hf_model, prompts, tokenizer, is_hf=True, draft_model=draft_model, **generation_kwargs)

    print(f"Expected output: {outputs_expected}")


    output_token_ids, outputs_actual = get_generate_outputs(
        neuron_model, prompts, tokenizer, is_hf=False, draft_model=draft_model, **generation_kwargs
    )
    print(f"Actual output  : {outputs_actual}")

    pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token) if tokenizer else 0
    output_token_ids = output_token_ids[output_token_ids != pad_token_id]
    expected_token_ids = expected_token_ids[expected_token_ids != pad_token_id]
    if draft_model is not None:
        # Handle corner scenario where last few tokens are not generated as part of speculation.
        assert (
            abs(expected_token_ids.shape[-1] - output_token_ids.shape[-1]) <= neuron_config.speculation_length
        ), "Unexpected number of tokens generated by target model"
        tokens_to_compare = min(expected_token_ids.shape[-1], output_token_ids.shape[-1])
        expected_token_ids = expected_token_ids[: tokens_to_compare]
        output_token_ids = output_token_ids[: tokens_to_compare]

    device = "neuron"
    assert torch.equal(
        output_token_ids, expected_token_ids
    ), f"\nActual: ({device}) {output_token_ids} \nExpected (hf-cpu): {expected_token_ids}"
    print(f"The output from Neuronx NxD on {device} is accurate!")


# TODO: comment out for now given none of test is using logits check
# def check_accuracy_logits(
#     self,
#     traced_model: PreTrainedModel,
#     batch_size: int,
#     max_length: int,
#     expected_logits: torch.Tensor = None,
#     divergence_difference_tol: float = 0.001,
#     remove_shift: bool = True,
#     tol_map: dict = None,
# ):
#     if traced_model.neuron_config.on_device_sampling:
#         raise ValueError("Logits validation is not supported with on-device sampling.")

#     prompts = [TEST_PROMPT] * batch_size
#     tokenizer = self.load_tokenizer()
#     inputs = tokenizer(prompts, padding=True, return_tensors="pt")

#     if not expected_logits:
#         # logit_validation assumes greedy sampling
#         expected_outputs, _ = self.generate_with_hf(
#             prompts, max_length, do_sample=False, output_logits=True, return_dict_in_generate=True,
#         )
#         expected_logits = torch.stack(expected_outputs.logits)
#     expected_token_ids = expected_logits.argmax(dim=2).T
#     expected_tokens = tokenizer.batch_decode(
#         expected_token_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
#     )
#     print("Expected Output: ", expected_tokens, expected_token_ids)
#     print("Expected Logits Shape: ", expected_logits.shape)

#     def generate_logits(model, tokenizer, input_ids):
#         prompt = tokenizer.batch_decode(input_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
#         actual_outputs, actual_tokens = self.generate_on_neuron(
#             prompt, traced_model, do_sample=True, output_logits=True, return_dict_in_generate=True,
#             max_length=max_length
#         )
#         actual_logits = torch.stack(actual_outputs.logits)
#         actual_token_ids = actual_logits.argmax(dim=2).T
#         print("Actual Output: ", actual_tokens, actual_token_ids)
#         print("Actual Logits Shape: ", actual_logits.shape)
#         model.reset()
#         return actual_logits

#     generate_fn = partial(generate_logits, traced_model, tokenizer)
#     passed, result, status_msg = logit_validation(inputs.input_ids,
#                                                     generate_fn,
#                                                     expected_logits,
#                                                     divergence_difference_tol=divergence_difference_tol,
#                                                     tol_map=tol_map,
#                                                     pad_token_id=tokenizer.pad_token_id,
#                                                     padding_side=tokenizer.padding_side)
#     assert passed, status_msg
#     print("Passed logits validation")